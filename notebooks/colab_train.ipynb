{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neural_decoder_colab_title"
   },
   "source": [
    "# Neural Sequence Decoder - Colab Training\n",
    "\n",
    "This notebook allows you to train the neural sequence decoder model on Google Colab.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload your project folder to Colab (compress as .zip first)\n",
    "2. Run the setup cell to install dependencies and extract files\n",
    "3. Configure training parameters if needed\n",
    "4. Run training\n",
    "5. Download the trained model files\n",
    "\n",
    "**Note**: Make sure your data file `ptDecoder_ctc.pkl` is in the `data/pickledData/` folder of your uploaded project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup Environment and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "setup_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab\n",
      "Project directory: /code\n",
      "Current working directory: /code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Google Colab\")\n",
    "\n",
    "# Set up project directory\n",
    "if IN_COLAB:\n",
    "    # In Colab, look for uploaded zip file\n",
    "    project_zips = [f for f in os.listdir('/content') if f.endswith('.zip')]\n",
    "    if project_zips:\n",
    "        project_zip = project_zips[0]\n",
    "        print(f\"Found project zip: {project_zip}\")\n",
    "        \n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(f'/content/{project_zip}', 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content')\n",
    "        \n",
    "        # Find the extracted project directory\n",
    "        extracted_dirs = [d for d in os.listdir('/content') if os.path.isdir(f'/content/{d}') and d != 'sample_data']\n",
    "        if extracted_dirs:\n",
    "            project_dir = f'/content/{extracted_dirs[0]}'\n",
    "        else:\n",
    "            project_dir = '/content'\n",
    "    else:\n",
    "        print(\"No zip file found. Please upload your project as a zip file.\")\n",
    "        print(\"Expected structure: your_project.zip containing the neural_seq_decoder_pytorch folder\")\n",
    "        project_dir = '/content'\n",
    "else:\n",
    "    # Local development - assume we're in the notebooks directory\n",
    "    project_dir = str(Path.cwd().parent)\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "os.chdir(project_dir)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = os.path.join(project_dir, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local development mode - assuming dependencies are installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All dependencies imported successfully\n",
      "PyTorch version: 1.13.1+cu117\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# Install the package and its dependencies\n",
    "if IN_COLAB:\n",
    "    !pip install -e .\n",
    "else:\n",
    "    # For local development, assume dependencies are already installed\n",
    "    print(\"Local development mode - assuming dependencies are installed\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from neural_decoder.neural_decoder_trainer import trainModel\n",
    "    from neural_decoder.model import GRUDecoder\n",
    "    print(\"✓ All dependencies imported successfully\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please check that the package was installed correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 2. Verify Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for data at: /code/data/pickledData/ptDecoder_ctc.pkl\n",
      "✓ Data file found! Size: 3.26 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if data file exists\n",
    "data_path = os.path.join(project_dir, 'data', 'pickledData', 'ptDecoder_ctc.pkl')\n",
    "print(f\"Looking for data at: {data_path}\")\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    file_size = os.path.getsize(data_path) / (1024**3)  # Size in GB\n",
    "    print(f\"✓ Data file found! Size: {file_size:.2f} GB\")\n",
    "else:\n",
    "    print(\"❌ Data file not found!\")\n",
    "    print(\"Please ensure your uploaded project contains:\")\n",
    "    print(\"  data/pickledData/ptDecoder_ctc.pkl\")\n",
    "    print(\"\")\n",
    "    print(\"You can create this file using the formatCompetitionData.ipynb notebook\")\n",
    "    \n",
    "    # List what's actually in the data directory\n",
    "    data_dir = os.path.join(project_dir, 'data')\n",
    "    if os.path.exists(data_dir):\n",
    "        print(f\"Contents of {data_dir}:\")\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            level = root.replace(data_dir, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files:\n",
    "                print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure your training parameters here. The default settings are for full training (may take several hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode: quick\n",
      "Model output directory: /code/outputs/models/colab_trained_model\n",
      "Log directory: /code/logs/colab_training\n",
      "Using QUICK training configuration (500 batches, ~30 minutes)\n",
      "\n",
      "Training parameters:\n",
      "  seqLen: 150\n",
      "  maxTimeSeriesLen: 1200\n",
      "  batchSize: 32\n",
      "  lrStart: 0.02\n",
      "  lrEnd: 0.02\n",
      "  nUnits: 512\n",
      "  nBatch: 100\n",
      "  nLayers: 3\n",
      "  seed: 0\n",
      "  nClasses: 40\n",
      "  nInputFeatures: 256\n",
      "  dropout: 0.4\n",
      "  whiteNoiseSD: 0.8\n",
      "  constantOffsetSD: 0.2\n",
      "  gaussianSmoothWidth: 2.0\n",
      "  strideLen: 4\n",
      "  kernelLen: 32\n",
      "  bidirectional: True\n",
      "  l2_decay: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# You can modify these parameters as needed\n",
    "\n",
    "# Choose training mode: 'full' or 'quick'\n",
    "TRAINING_MODE = 'quick'  # Change to 'quick' for faster training with smaller model\n",
    "\n",
    "# Set up output directories\n",
    "output_dir = os.path.join(project_dir, 'outputs', 'models', 'colab_trained_model')\n",
    "log_dir = os.path.join(project_dir, 'logs', 'colab_training')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Training mode: {TRAINING_MODE}\")\n",
    "print(f\"Model output directory: {output_dir}\")\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "\n",
    "# Training arguments\n",
    "if TRAINING_MODE == 'full':\n",
    "    # Full training configuration (from train_rnn_full.sh)\n",
    "    args = {\n",
    "        'outputDir': output_dir,\n",
    "        'datasetPath': data_path,\n",
    "        'seqLen': 150,\n",
    "        'maxTimeSeriesLen': 1200,\n",
    "        'batchSize': 64,\n",
    "        'lrStart': 0.02,\n",
    "        'lrEnd': 0.02,\n",
    "        'nUnits': 1024,\n",
    "        'nBatch': 10000,  # This will take several hours\n",
    "        'nLayers': 5,\n",
    "        'seed': 0,\n",
    "        'nClasses': 40,\n",
    "        'nInputFeatures': 256,\n",
    "        'dropout': 0.4,\n",
    "        'whiteNoiseSD': 0.8,\n",
    "        'constantOffsetSD': 0.2,\n",
    "        'gaussianSmoothWidth': 2.0,\n",
    "        'strideLen': 4,\n",
    "        'kernelLen': 32,\n",
    "        'bidirectional': True,\n",
    "        'l2_decay': 1e-5\n",
    "    }\n",
    "    print(\"Using FULL training configuration (10,000 batches, ~several hours)\")\n",
    "else:\n",
    "    # Quick training configuration (from train_rnn_quick.sh)\n",
    "    args = {\n",
    "        'outputDir': output_dir,\n",
    "        'datasetPath': data_path,\n",
    "        'seqLen': 150,\n",
    "        'maxTimeSeriesLen': 1200,\n",
    "        'batchSize': 32,\n",
    "        'lrStart': 0.02,\n",
    "        'lrEnd': 0.02,\n",
    "        'nUnits': 512,\n",
    "        'nBatch': 100,  # Much faster for testing\n",
    "        'nLayers': 3,\n",
    "        'seed': 0,\n",
    "        'nClasses': 40,\n",
    "        'nInputFeatures': 256,\n",
    "        'dropout': 0.4,\n",
    "        'whiteNoiseSD': 0.8,\n",
    "        'constantOffsetSD': 0.2,\n",
    "        'gaussianSmoothWidth': 2.0,\n",
    "        'strideLen': 4,\n",
    "        'kernelLen': 32,\n",
    "        'bidirectional': True,\n",
    "        'l2_decay': 1e-5\n",
    "    }\n",
    "    print(\"Using QUICK training configuration (500 batches, ~30 minutes)\")\n",
    "\n",
    "print(\"\\nTraining parameters:\")\n",
    "for key, value in args.items():\n",
    "    if key not in ['outputDir', 'datasetPath']:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 4. Run Training\n",
    "\n",
    "This cell will start the training process and **automatically download** the trained model when complete.\n",
    "\n",
    "**Features:**\n",
    "- Monitor training progress with real-time updates\n",
    "- **Auto-download**: Model files are automatically downloaded when training finishes\n",
    "- Multiple download fallbacks ensure you get your model files\n",
    "- Training duration tracking and file size reporting\n",
    "\n",
    "**Warning**: Full training may take several hours. Make sure your Colab session stays active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data file found: /code/data/pickledData/ptDecoder_ctc.pkl\n",
      "Starting training at 2025-08-17 21:08:03\n",
      "Training mode: quick\n",
      "Number of batches: 100\n",
      "\n",
      "==================================================\n",
      "TRAINING STARTED\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/code/src/neural_decoder/augmentations.py:91: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return self.conv(input, weight=self.weight, groups=self.groups, padding=\"same\")\n",
      "/code/src/neural_decoder/neural_decoder_trainer.py:177: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(pred[iterIdx, 0 : adjustedLens[iterIdx], :]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, ctc loss: 6.301495, cer: 0.934836, time/batch:   0.028\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED SUCCESSFULLY!\n",
      "==================================================\n",
      "Training duration: 00:00:33\n",
      "Model saved to: /code/outputs/models/colab_trained_model\n",
      "\n",
      "Generated files:\n",
      "  trainingStats (0.0 MB)\n",
      "  args (0.0 MB)\n",
      "  modelWeights (150.3 MB)\n",
      "\n",
      "==================================================\n",
      "AUTO-SAVING TRAINED MODEL TO PERSISTENT STORAGE...\n",
      "==================================================\n",
      "Creating zip file: trained_model_quick_20250817_210836.zip\n",
      "  Added: trainingStats\n",
      "  Added: args\n",
      "  Added: modelWeights\n",
      "\n",
      "Zip file created: trained_model_quick_20250817_210836.zip (138.1 MB)\n",
      "🔗 Creating download link...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/code/trained_model_quick_20250817_210836.zip' target='_blank'>/code/trained_model_quick_20250817_210836.zip</a><br>"
      ],
      "text/plain": [
       "/code/trained_model_quick_20250817_210836.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download link created! Click above to download.\n",
      "\n",
      "==================================================\n",
      "TRAINING AND AUTO-DOWNLOAD COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Verify data exists before starting training\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"❌ Cannot start training: Data file not found!\")\n",
    "    print(f\"Expected: {data_path}\")\n",
    "else:\n",
    "    print(f\"✓ Data file found: {data_path}\")\n",
    "    print(f\"Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training mode: {TRAINING_MODE}\")\n",
    "    print(f\"Number of batches: {args['nBatch']}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING STARTED\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Import and run the training function\n",
    "        from neural_decoder.neural_decoder_trainer import trainModel\n",
    "        trainModel(args)\n",
    "        \n",
    "        # Training completed successfully\n",
    "        end_time = time.time()\n",
    "        training_duration = end_time - start_time\n",
    "        hours = int(training_duration // 3600)\n",
    "        minutes = int((training_duration % 3600) // 60)\n",
    "        seconds = int(training_duration % 60)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Training duration: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "        print(f\"Model saved to: {output_dir}\")\n",
    "        \n",
    "        # List generated files\n",
    "        if os.path.exists(output_dir):\n",
    "            print(\"\\nGenerated files:\")\n",
    "            for file in os.listdir(output_dir):\n",
    "                file_path = os.path.join(output_dir, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    size_mb = os.path.getsize(file_path) / (1024**2)\n",
    "                    print(f\"  {file} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # AUTO-SAVE: Ensure model is saved persistently even if you're away\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"AUTO-SAVING TRAINED MODEL TO PERSISTENT STORAGE...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Import download libraries safely\n",
    "        try:\n",
    "            from google.colab import files as colab_files\n",
    "            COLAB_FILES_AVAILABLE = True\n",
    "        except ImportError:\n",
    "            COLAB_FILES_AVAILABLE = False\n",
    "            colab_files = None\n",
    "        \n",
    "        try:\n",
    "            from IPython.display import FileLink, display\n",
    "            FILELINK_AVAILABLE = True\n",
    "        except ImportError:\n",
    "            FILELINK_AVAILABLE = False\n",
    "            FileLink = None\n",
    "        \n",
    "        try:\n",
    "            import base64\n",
    "            from IPython.display import HTML\n",
    "            HTML_DOWNLOAD_AVAILABLE = True\n",
    "        except ImportError:\n",
    "            HTML_DOWNLOAD_AVAILABLE = False\n",
    "            HTML = None\n",
    "        \n",
    "        # Create zip file for download\n",
    "        from datetime import datetime\n",
    "        zip_filename = f'trained_model_{TRAINING_MODE}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            zip_path = os.path.join('/content', zip_filename)\n",
    "        else:\n",
    "            zip_path = os.path.join(project_dir, zip_filename)\n",
    "        \n",
    "        print(f\"Creating zip file: {zip_filename}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # Add all files from the output directory\n",
    "            for root, dirs, files in os.walk(output_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    archive_path = os.path.relpath(file_path, output_dir)\n",
    "                    zipf.write(file_path, archive_path)\n",
    "                    print(f\"  Added: {archive_path}\")\n",
    "            \n",
    "            # Also add training logs if they exist\n",
    "            if os.path.exists(log_dir):\n",
    "                for root, dirs, files in os.walk(log_dir):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        archive_path = os.path.join('logs', os.path.relpath(file_path, log_dir))\n",
    "                        zipf.write(file_path, archive_path)\n",
    "                        print(f\"  Added: {archive_path}\")\n",
    "        \n",
    "        zip_size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "        print(f\"\\nZip file created: {zip_filename} ({zip_size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Try multiple download methods\n",
    "        download_success = False\n",
    "        \n",
    "        # Method 1: Google Colab files (primary)\n",
    "        if IN_COLAB and COLAB_FILES_AVAILABLE:\n",
    "            try:\n",
    "                print(\"🚀 Auto-downloading via Google Colab...\")\n",
    "                colab_files.download(zip_path)\n",
    "                print(\"✅ AUTO-DOWNLOAD COMPLETED!\")\n",
    "                download_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Colab auto-download failed: {e}\")\n",
    "        \n",
    "        # Method 2: IPython FileLink (backup)\n",
    "        if not download_success and FILELINK_AVAILABLE:\n",
    "            try:\n",
    "                print(\"🔗 Creating download link...\")\n",
    "                link = FileLink(zip_path)\n",
    "                display(link)\n",
    "                print(\"✅ Download link created! Click above to download.\")\n",
    "                download_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"❌ FileLink failed: {e}\")\n",
    "        \n",
    "        # Method 3: HTML download (for small files)\n",
    "        if not download_success and HTML_DOWNLOAD_AVAILABLE and zip_size_mb < 50:\n",
    "            try:\n",
    "                print(\"🎯 Creating HTML download button...\")\n",
    "                with open(zip_path, 'rb') as f:\n",
    "                    zip_data = f.read()\n",
    "                \n",
    "                b64_data = base64.b64encode(zip_data).decode()\n",
    "                download_html = f'''\n",
    "                <div style=\"text-align: center; margin: 20px;\">\n",
    "                    <a download=\"{zip_filename}\" href=\"data:application/zip;base64,{b64_data}\" \n",
    "                       style=\"background-color: #4CAF50; color: white; padding: 15px 30px; \n",
    "                              text-decoration: none; border-radius: 8px; font-weight: bold; \n",
    "                              font-size: 16px; display: inline-block;\">\n",
    "                       🎉 DOWNLOAD TRAINED MODEL ({zip_size_mb:.1f} MB)\n",
    "                    </a>\n",
    "                </div>\n",
    "                '''\n",
    "                display(HTML(download_html))\n",
    "                print(\"✅ Download button created! Click the button above.\")\n",
    "                download_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"❌ HTML download failed: {e}\")\n",
    "        \n",
    "        # Fallback instructions\n",
    "        if not download_success:\n",
    "            print(\"\\n⚠️  Auto-download failed. Manual options:\")\n",
    "            print(f\"📁 File location: {zip_path}\")\n",
    "            if IN_COLAB:\n",
    "                print(\"   • Use Colab file browser (left sidebar) to download\")\n",
    "                print(\"   • Or run: !cp '{}' /content/download.zip\".format(zip_path))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING AND AUTO-DOWNLOAD COMPLETE!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## 5. Manual Download (Optional)\n",
    "\n",
    "**Note**: The training cell above automatically downloads your model when training completes!\n",
    "\n",
    "Use this cell only if:\n",
    "- Auto-download failed during training\n",
    "- You want to re-download the model files\n",
    "- You need to download a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IN_COLAB \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Create a zip file with all model files\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     zip_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAINING_MODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "if IN_COLAB and os.path.exists(output_dir):\n",
    "    # Create a zip file with all model files\n",
    "    zip_filename = f'trained_model_{TRAINING_MODE}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
    "    zip_path = os.path.join('/content', zip_filename)\n",
    "    \n",
    "    print(f\"Creating zip file: {zip_filename}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add all files from the output directory\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Create archive path relative to output_dir\n",
    "                archive_path = os.path.relpath(file_path, output_dir)\n",
    "                zipf.write(file_path, archive_path)\n",
    "                print(f\"  Added: {archive_path}\")\n",
    "        \n",
    "        # Also add training logs if they exist\n",
    "        if os.path.exists(log_dir):\n",
    "            for root, dirs, files in os.walk(log_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    archive_path = os.path.join('logs', os.path.relpath(file_path, log_dir))\n",
    "                    zipf.write(file_path, archive_path)\n",
    "                    print(f\"  Added: {archive_path}\")\n",
    "    \n",
    "    # Get zip file size\n",
    "    zip_size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "    print(f\"\\nZip file created: {zip_filename} ({zip_size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Download the zip file\n",
    "    print(\"Starting download...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"Download completed!\")\n",
    "    \n",
    "elif not IN_COLAB:\n",
    "    print(\"Not running in Colab - model files are already saved locally at:\")\n",
    "    print(f\"  {output_dir}\")\n",
    "    if os.path.exists(output_dir):\n",
    "        print(\"\\nModel files:\")\n",
    "        for file in os.listdir(output_dir):\n",
    "            print(f\"  {file}\")\n",
    "else:\n",
    "    print(\"❌ No trained model found to download.\")\n",
    "    print(\"Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage_section"
   },
   "source": [
    "## 6. Using the Trained Model\n",
    "\n",
    "After downloading, you can load and use the trained model in your local environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_usage_example"
   },
   "outputs": [],
   "source": [
    "# Example of how to load the trained model\n",
    "# (This cell is for reference - run this in your local environment after downloading)\n",
    "\n",
    "print(\"Example code to load the trained model:\")\n",
    "print(\"\")\n",
    "print(\"```python\")\n",
    "print(\"from neural_decoder.neural_decoder_trainer import loadModel\")\n",
    "print(\"\")\n",
    "print(\"# Load the trained model\")\n",
    "print(f\"model = loadModel('{output_dir}', device='cuda')\")\n",
    "print(\"\")\n",
    "print(\"# The model is now ready for inference\")\n",
    "print(\"# Use model.forward(neural_input, day_idx) for predictions\")\n",
    "print(\"```\")\n",
    "print(\"\")\n",
    "print(\"Key files in the model directory:\")\n",
    "print(\"  - modelWeights: PyTorch model state dict\")\n",
    "print(\"  - args: Training arguments (pickled)\")\n",
    "print(\"  - trainingStats: Training statistics and metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting_section"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Data file not found**: Make sure your uploaded zip contains `data/pickledData/ptDecoder_ctc.pkl`\n",
    "2. **Out of memory**: Try reducing batch size or switching to 'quick' training mode\n",
    "3. **Session timeout**: For full training, consider using Colab Pro for longer sessions\n",
    "4. **Import errors**: Make sure the package was installed correctly in the setup cell\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- Monitor GPU usage: `!nvidia-smi`\n",
    "- Check disk space: `!df -h`\n",
    "- For long training, consider saving intermediate checkpoints\n",
    "- Use 'quick' mode first to test everything works before full training"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
